# 使用词图模型生成文档集主题概览视图

## 目标
文本分析处理的对象是包含大量文档的文档集，无法人工一一翻阅的情况下，通过机器学习手段自动构建能表达文档集内容的主题概览视图，需要建立文档的表示模型。本研究从文档关键词共现关联的角度，探索通过词图(Term Graph)模型表达文档集的主题聚集、热点发现等。

## 算法设计

### 文档模型
词(Term)是文档中包含语义的最小组成单元，文档向量由文档中的词构成。不同词性的词在语义表达上的作用是不同的，虚词（副词、介词、连词等）是没有完整意义的词汇，只有语法意义或功能意义，不作为单独的语法成分。实词有实在意义，能够单独充当句子成分一般能单独回答问题，包括名词、动词、形容词、数词、量词、代词六类。因此，本研究中文档向量由文档中的名词和动词构成。

### 事件模型
事件e被定义为6元组格式：

e = (A, O, T, V, P, L)

A是事件中的动作，O是事件中的对象，T是事件发生的时间，V是事件发生的环境（包括自然环境和社会环境），P是事件中动作执行过程的断言，L是语言表达式。

事件触发词(Event Trigger Word)，统计上表明词性主要是名词、动词、动名词。

[事件本体以及突发事件语料库--CEC(Chinese Emergency Corpus)](http://blog.csdn.net/shijiebei2009/article/details/44538257)

[中文突发事件语料库](https://github.com/shijiebei2009/CEC-Corpus)

[中文环境突发事件语料库](https://github.com/shijiebei2009/CEEC-Corpus)

[基于CEC语料库挖掘要素识别规则，对新闻报道类生语料进行自动标注](https://github.com/shijiebei2009/CEC-Automatic-Annotation)

## 实验

## 结论

## 相关工作

### 词法分析

本研究选用的是开源的Jieba分词。目前主流分词工具通常包含了词性标注功能。Jieba分词完成词性标注的过程如下：

```
>>> import jieba.posseg as pseg
>>> words = pseg.cut(u'我爱北京天安门')
>>> for w in words:
...   print w.word, w.flag
...
我 r
爱 v
北京 ns
天安门 ns
```

### 关键词抽取
