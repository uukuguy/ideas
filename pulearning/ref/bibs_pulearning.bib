
@article{Mordelet2014201,
title = "A bagging \{SVM\} to learn from positive and unlabeled examples ",
journal = "Pattern Recognition Letters ",
volume = "37",
number = "",
pages = "201 - 209",
year = "2014",
note = "Partially Supervised Learning for Pattern Recognition ",
issn = "0167-8655",
doi = "http://dx.doi.org/10.1016/j.patrec.2013.06.010",
url = "http://www.sciencedirect.com.zhongjivip.net/science/article/pii/S0167865513002432",
author = "F. Mordelet and J.-P. Vert",
keywords = "\{PU\} learning",
keywords = "Bagging",
keywords = "SVM ",
abstract = "Abstract We consider the problem of learning a binary classifier from a training set of positive and unlabeled examples, both in the inductive and in the transductive setting. This problem, often referred to as \{PU\} learning, differs from the standard supervised classification problem by the lack of negative examples in the training set. It corresponds to an ubiquitous situation in many applications such as information retrieval or gene ranking, when we have identified a set of data of interest sharing a particular property, and we wish to automatically retrieve additional data sharing the same property among a large and easily available pool of unlabeled data. We propose a new method for \{PU\} learning with a conceptually simple implementation based on bootstrap aggregating (bagging) techniques: the algorithm iteratively trains many binary classifiers to discriminate the known positive examples from random subsamples of the unlabeled set, and averages their predictions. We show theoretically and experimentally that the method can match and even outperform the performance of state-of-the-art methods for \{PU\} learning, particularly when the number of positive examples is limited and the fraction of negatives among the unlabeled examples is small. The proposed method can also run considerably faster than state-of-the-art methods, particularly when the set of unlabeled examples is large. "
}

@INPROCEEDINGS{1250918, 
author={Bing Liu and Yang Dai and Li, X. and Wee Sun Lee and Yu, P.S.}, 
booktitle={Data Mining, 2003. ICDM 2003. Third IEEE International Conference on}, 
title={Building text classifiers using positive and unlabeled examples}, 
year={2003}, 
pages={179-186}, 
abstract={We study the problem of building text classifiers using positive and unlabeled examples. The key feature of this problem is that there is no negative example for learning. Recently, a few techniques for solving this problem were proposed in the literature. These techniques are based on the same idea, which builds a classifier in two steps. Each existing technique uses a different method for each step. We first introduce some new methods for the two steps, and perform a comprehensive evaluation of all possible combinations of methods of the two steps. We then propose a more principled approach to solving the problem based on a biased formulation of SVM, and show experimentally that it is more accurate than the existing techniques.}, 
keywords={Bayes methods;belief networks;pattern classification;support vector machines;text analysis;SVM;positive example;text classifier;unlabeled example;Biomedical engineering;Computer science;Iterative algorithms;Labeling;Niobium;Performance evaluation;Sun;Support vector machine classification;Support vector machines;Text categorization}, 
doi={10.1109/ICDM.2003.1250918}, 
month={Nov},}

@paper{AAAI113583,
	author = {Tao Liu and Xiaoyong Du and Yongdong Xu and Minghui Li and Xiaolong Wang},
	title = {Partially Supervised Text Classification with Multi-Level Examples},
	conference = {AAAI Conference on Artificial Intelligence},
	year = {2011},
	keywords = {},
	abstract = {Partially supervised text classification has received great research attention since it only uses positive and unlabeled examples as training data. This problem can be solved by automatically labeling some negative (and more positive) examples from unlabeled examples before training a text classifier. But it is difficult to guarantee both high quality and quantity of the new labeled examples. In this paper, a multi-level example based learning method for partially supervised text classification is proposed, which can make full use of all unlabeled examples. A heuristic method is proposed to assign possible labels to unlabeled examples and partition them into multiple levels according to their labeling confidence. A text classifier is trained on these multi-level examples using weighted support vector machines. Experiments show that the multi-level example based learning method is effective for partially supervised text classification, and outperforms the existing popular methods such as Biased-SVM, ROC-SVM, S-EM and WL.},

	url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/view/3583}
}

@inproceedings{li2009positive,
  title={Positive Unlabeled Learning for Data Stream Classification.},
  author={Li, Xiaoli and Philip, S Yu and Liu, Bing and Ng, See-Kiong},
  booktitle={SDM},
  volume={9},
  pages={257--268},
  year={2009},
  organization={SIAM}
}

@inproceedings{xiao2011similarity,
  title={Similarity-based approach for positive and unlabeled learning},
  author={Xiao, Yanshan and Liu, Bing and Yin, Jie and Cao, Longbing and Zhang, Chengqi and Hao, Zhifeng},
  booktitle={IJCAI Proceedings-International Joint Conference on Artificial Intelligence},
  volume={22},
  number={1},
  pages={1577},
  year={2011}
}

@inproceedings{li2003learning,
  title={Learning to classify texts using positive and unlabeled data},
  author={Li, Xiaoli and Liu, Bing},
  booktitle={IJCAI},
  volume={3},
  pages={587--592},
  year={2003}
}

@article{NI-OVERSAMPLING,
	Author = {陶新民 and 徐晶 and 童智靖 and 刘玉 },
	Journal = {控制与决策},
	Number = {TP18},
	Title = {不均衡数据下基于阴性免疫的过抽样新算法},
	Volume = {25(6)},
	Year = {2010}}

